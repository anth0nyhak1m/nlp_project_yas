{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import unicodedata\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from transformers import AutoModel, BertTokenizerFast\n",
    "import torch\n",
    "from transformers import AdamW\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Part 1: Preparation$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3852/4231496941.py:6: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data = true_data.append(fake_data).sample(frac=1).reset_index().drop(columns=['index', 'subject', 'date', 'text'])\n"
     ]
    }
   ],
   "source": [
    "# Import and merge data\n",
    "true_data = pd.read_csv('data/true.csv')\n",
    "fake_data = pd.read_csv('data/fake.csv')\n",
    "true_data[\"label\"] = 1\n",
    "fake_data[\"label\"] = 0\n",
    "data = true_data.append(fake_data).sample(frac=1).reset_index().drop(columns=['index', 'subject', 'date', 'text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need from util.py: a helper function to remover \"Reuters\" from real news without modifying the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>America’s Stepdad Tim Kaine Just Burned Trump...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Trump Just Issued A New But Utterly USELESS T...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DEM PARTY OFFICIAL, Chair Of Black Caucus, Ber...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MUSLIM SCHOLAR CRITICIZES OBAMA…Explains Why A...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>US Presidential Debates Much More Corrupt Than...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  label\n",
       "0   America’s Stepdad Tim Kaine Just Burned Trump...      0\n",
       "1   Trump Just Issued A New But Utterly USELESS T...      0\n",
       "2  DEM PARTY OFFICIAL, Chair Of Black Caucus, Ber...      0\n",
       "3  MUSLIM SCHOLAR CRITICIZES OBAMA…Explains Why A...      0\n",
       "4  US Presidential Debates Much More Corrupt Than...      0"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Part 1: Preparation$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training (70%), validation (15%) and testing (15%)\n",
    "\n",
    "# To make the process faster, we only take a small portion of data (1000 samples)\n",
    "data = data[:5000]\n",
    "\n",
    "train_text, temp_text, train_labels, temp_labels = train_test_split(data['title'], data['label'], \n",
    "                                                                    random_state=2018, \n",
    "                                                                    test_size=0.3, \n",
    "                                                                    stratify=data['label'])\n",
    "\n",
    "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels, \n",
    "                                                                random_state=2018, \n",
    "                                                                test_size=0.5, \n",
    "                                                                stratify=temp_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    The dataset has 5000 pieces of news in total.\n",
      "    The portion of real news in this dataset is 0.4756\n",
      "    The portion of real news in this dataset is 0.5244\n",
      "\n",
      "    Training data size: 3500,\n",
      "    Validation data size: 750,\n",
      "    Testing data size: 750.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# Check the balance of different classifications\n",
    "\n",
    "print('''\n",
    "    The dataset has {} pieces of news in total.\n",
    "    The portion of real news in this dataset is {}\n",
    "    The portion of real news in this dataset is {}'''.format(\n",
    "        len(data),\n",
    "        len(data[data['label'] == 1])/len(data),\n",
    "        len(data[data['label'] == 0])/len(data),\n",
    "    ))\n",
    "\n",
    "print('''\n",
    "    Training data size: {},\n",
    "    Validation data size: {},\n",
    "    Testing data size: {}.\n",
    "    '''.format(len(train_text), len(val_text), len(test_text))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Part 2: Model Building$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.449142857142856"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Measure the average length of the context (title)\n",
    "np.mean([len(i.split()) for i in train_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anthonyhakim/anaconda3/envs/nlp/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2263: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Now that we have the average length of our sample, we tokenize them into an object for BERT to learn\n",
    "\n",
    "L = 20\n",
    "\n",
    "def tokenize(text, max_len):\n",
    "    '''\n",
    "    A function to turn English text into a token by the max length\n",
    "    Inputs: \n",
    "      text (str): the text to process\n",
    "      max_len (int): the maximum length of a token\n",
    "    Returns:\n",
    "      tk: an BERT Encoding object to train the model\n",
    "    '''\n",
    "\n",
    "    tk = tokenizer.batch_encode_plus(\n",
    "        text.tolist(),\n",
    "        max_length = max_len,\n",
    "        pad_to_max_length=True,\n",
    "        truncation=True\n",
    "        )\n",
    "\n",
    "    return tk\n",
    "\n",
    "train_tk = tokenize(train_text, L)\n",
    "val_tk = tokenize(val_text, L)\n",
    "test_tk = tokenize(test_text, L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizing every sentense, create a quantified database for machine learning\n",
    "\n",
    "train_seq = torch.tensor(train_tk['input_ids'])\n",
    "train_mask = torch.tensor(train_tk['attention_mask'])\n",
    "train_y = torch.tensor(train_labels.tolist())\n",
    "\n",
    "val_seq = torch.tensor(val_tk['input_ids'])\n",
    "val_mask = torch.tensor(val_tk['attention_mask'])\n",
    "val_y = torch.tensor(val_labels.tolist())\n",
    "\n",
    "test_seq = torch.tensor(test_tk['input_ids'])\n",
    "test_mask = torch.tensor(test_tk['attention_mask'])\n",
    "test_y = torch.tensor(test_labels.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " America’s Stepdad Tim Kaine Just Burned Trump HARD On Colbert\n",
      "[101, 28514, 17248, 4187, 2342, 2005, 1057, 1012, 1055, 1012, 4040, 4335, 1010, 2231, 4804, 102, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# An intuition: what we are doing: Vectorizing every sentense\n",
    "print(train_text[0])\n",
    "print(train_tk['input_ids'][0])\n",
    "print(train_tk['attention_mask'][0])\n",
    "print(train_labels.tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert\n",
    "# In each epoch of training, the model randomly select training datasets\n",
    "\n",
    "batch_size = 150\n",
    "\n",
    "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in bert.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "class BERT_Arch(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, bert):\n",
    "      \n",
    "        super(BERT_Arch, self).__init__()\n",
    "\n",
    "        self.bert = bert \n",
    "        self.dropout = torch.nn.Dropout(0.1)\n",
    "        self.relu =  torch.nn.ReLU()\n",
    "        self.fc1 = torch.nn.Linear(768,512)\n",
    "        self.fc2 = torch.nn.Linear(512,2)\n",
    "        self.softmax = torch.nn.LogSoftmax(dim=1)\n",
    "\n",
    "    #define the forward pass\n",
    "    def forward(self, sent_id, mask):\n",
    "        cls_hs = self.bert(sent_id, attention_mask=mask)['pooler_output']\n",
    "        x = self.fc1(cls_hs)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anthonyhakim/anaconda3/envs/nlp/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = BERT_Arch(bert)\n",
    "\n",
    "# define the optimizer\n",
    "optimizer = AdamW(model.parameters(),lr = 1e-5) # learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Part3: Training$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights: [0.95367847 1.05105105]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "class_weights = compute_class_weight(\n",
    "                                        class_weight = \"balanced\",\n",
    "                                        classes = np.unique(train_labels),\n",
    "                                        y = train_labels                                                    \n",
    "                                    )\n",
    "print(\"Class Weights:\",class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights= torch.tensor(class_weights,dtype=torch.float)\n",
    "\n",
    "cross_entropy  = torch.nn.NLLLoss(weight=weights) \n",
    "\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "  \n",
    "    model.train()\n",
    "\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "    \n",
    "    # empty list to save model predictions\n",
    "    total_preds=[]\n",
    "    \n",
    "    # iterate over batches\n",
    "    for step,batch in enumerate(train_dataloader):\n",
    "    \n",
    "        # progress update after every 50 batches.\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
    "        batch = [r for r in batch]\n",
    "        sent_id, mask, labels = batch\n",
    "        model.zero_grad()        \n",
    "        preds = model(sent_id, mask)\n",
    "        loss = cross_entropy(preds, labels)\n",
    "        total_loss = total_loss + loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        preds=preds.detach().cpu().numpy()\n",
    "        total_preds.append(preds)\n",
    "\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "    total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "    return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    \n",
    "    print(\"\\nEvaluating...\")\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "    total_preds = []\n",
    "    for step,batch in enumerate(val_dataloader):\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
    "\n",
    "        batch = [t for t in batch]\n",
    "\n",
    "        sent_id, mask, labels = batch\n",
    "\n",
    "        with torch.no_grad():\n",
    "            preds = model(sent_id, mask)\n",
    "            loss = cross_entropy(preds,labels)\n",
    "            total_loss = total_loss + loss.item()\n",
    "            preds = preds.detach().cpu().numpy()\n",
    "            total_preds.append(preds)\n",
    "\n",
    "    avg_loss = total_loss / len(val_dataloader) \n",
    "\n",
    "    # reshape the predictions in form of (number of samples, no. of classes)\n",
    "    total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "    return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1 / 10\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.686\n",
      "Validation Loss: 0.654\n",
      "\n",
      " Epoch 2 / 10\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.649\n",
      "Validation Loss: 0.618\n",
      "\n",
      " Epoch 3 / 10\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.625\n",
      "Validation Loss: 0.594\n",
      "\n",
      " Epoch 4 / 10\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.605\n",
      "Validation Loss: 0.580\n",
      "\n",
      " Epoch 5 / 10\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.599\n",
      "Validation Loss: 0.569\n",
      "\n",
      " Epoch 6 / 10\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.590\n",
      "Validation Loss: 0.560\n",
      "\n",
      " Epoch 7 / 10\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.578\n",
      "Validation Loss: 0.555\n",
      "\n",
      " Epoch 8 / 10\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.580\n",
      "Validation Loss: 0.547\n",
      "\n",
      " Epoch 9 / 10\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.569\n",
      "Validation Loss: 0.540\n",
      "\n",
      " Epoch 10 / 10\n",
      "\n",
      "Evaluating...\n",
      "\n",
      "Training Loss: 0.568\n",
      "Validation Loss: 0.539\n"
     ]
    }
   ],
   "source": [
    "best_valid_loss = float('inf')\n",
    "\n",
    "# empty lists to store training and validation loss of each epoch\n",
    "train_losses=[]\n",
    "valid_losses=[]\n",
    "\n",
    "#for each epoch\n",
    "for epoch in range(epochs):\n",
    "     \n",
    "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
    "\n",
    "    train_loss, _ = train()\n",
    "\n",
    "    valid_loss, _ = evaluate()\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
    "    print(f'Validation Loss: {valid_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load weights of best model\n",
    "path = 'saved_weights.pt'\n",
    "model.load_state_dict(torch.load(path))\n",
    "with torch.no_grad():\n",
    "    preds = model(test_seq, test_mask)\n",
    "    preds = preds.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Part4: Performance$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.56      0.68       394\n",
      "           1       0.65      0.91      0.76       356\n",
      "\n",
      "    accuracy                           0.73       750\n",
      "   macro avg       0.76      0.74      0.72       750\n",
      "weighted avg       0.77      0.73      0.72       750\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check precision, recall and f1-score \n",
    "preds = np.argmax(preds, axis = 1)\n",
    "print(classification_report(test_y, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(matrix):\n",
    "    tot =matrix[0][0] + matrix[0][1] + matrix[1][0] + matrix[1][1]\n",
    "    part = matrix[1][1] + matrix[0][0]\n",
    "    return part/tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[220  31]\n",
      " [174 325]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7266666666666667"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute a confusion maatrix on prediction results \n",
    "x = confusion_matrix(preds,test_y)\n",
    "print(x)\n",
    "accuracy(x)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8c4ea79c1a7284fa48037fd0778a218119e8be6c8dc0c499ebdccb12c3aa4e96"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
