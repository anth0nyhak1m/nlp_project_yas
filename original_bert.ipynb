{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import unicodedata\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoModel, BertTokenizerFast\n",
    "import torch\n",
    "from transformers import AdamW\n",
    "# from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Part 1: Preparation$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/user/21872/ipykernel_2951426/4231496941.py:6: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data = true_data.append(fake_data).sample(frac=1).reset_index().drop(columns=['index', 'subject', 'date', 'text'])\n"
     ]
    }
   ],
   "source": [
    "# Import and merge data\n",
    "true_data = pd.read_csv('data/true.csv')\n",
    "fake_data = pd.read_csv('data/fake.csv')\n",
    "true_data[\"label\"] = 1\n",
    "fake_data[\"label\"] = 0\n",
    "data = true_data.append(fake_data).sample(frac=1).reset_index().drop(columns=['index', 'subject', 'date', 'text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need from util.py: a helper function to remover \"Reuters\" from real news without modifying the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Campaign In Damage Control Mode After Trump J...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WATCH WHAT HAPPENS When Guy Makes Undercover V...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FEC Finds NUMEROUS Election Law Violations In...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>After U.S. veto, U.N. General Assembly to meet...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Oregon Won’t Prosecute Baby’s Abuser Because ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  label\n",
       "0   Campaign In Damage Control Mode After Trump J...      0\n",
       "1  WATCH WHAT HAPPENS When Guy Makes Undercover V...      0\n",
       "2   FEC Finds NUMEROUS Election Law Violations In...      0\n",
       "3  After U.S. veto, U.N. General Assembly to meet...      1\n",
       "4   Oregon Won’t Prosecute Baby’s Abuser Because ...      0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Part 1: Preparation$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training (70%), validation (15%) and testing (15%)\n",
    "\n",
    "# To make the process faster, we only take a small portion of data (1000 samples)\n",
    "data = data[:1000]\n",
    "\n",
    "train_text, temp_text, train_labels, temp_labels = train_test_split(data['title'], data['label'], \n",
    "                                                                    random_state=2018, \n",
    "                                                                    test_size=0.3, \n",
    "                                                                    stratify=data['label'])\n",
    "\n",
    "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels, \n",
    "                                                                random_state=2018, \n",
    "                                                                test_size=0.5, \n",
    "                                                                stratify=temp_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    The dataset has 1000 pieces of news in total.\n",
      "    The portion of real news in this dataset is 0.464\n",
      "    The portion of real news in this dataset is 0.536\n",
      "\n",
      "    Training data size: 700,\n",
      "    Validation data size: 150,\n",
      "    Testing data size: 150.\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# Check the balance of different classifications\n",
    "\n",
    "print('''\n",
    "    The dataset has {} pieces of news in total.\n",
    "    The portion of real news in this dataset is {}\n",
    "    The portion of real news in this dataset is {}'''.format(\n",
    "        len(data),\n",
    "        len(data[data['label'] == 1])/len(data),\n",
    "        len(data[data['label'] == 0])/len(data),\n",
    "    ))\n",
    "\n",
    "print('''\n",
    "    Training data size: {},\n",
    "    Validation data size: {},\n",
    "    Testing data size: {}.\n",
    "    '''.format(len(train_text), len(val_text), len(test_text))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Part 2: Model Building$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nAutoModel requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/home/yifu/nlp_project_yas/original_bert.ipynb Cell 11'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Blinux4.cs.uchicago.edu/home/yifu/nlp_project_yas/original_bert.ipynb#ch0000011vscode-remote?line=0'>1</a>\u001b[0m bert \u001b[39m=\u001b[39m AutoModel\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m'\u001b[39m\u001b[39mbert-base-uncased\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blinux4.cs.uchicago.edu/home/yifu/nlp_project_yas/original_bert.ipynb#ch0000011vscode-remote?line=1'>2</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m BertTokenizerFast\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m'\u001b[39m\u001b[39mbert-base-uncased\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/utils/import_utils.py:788\u001b[0m, in \u001b[0;36mDummyObject.__getattr__\u001b[0;34m(cls, key)\u001b[0m\n\u001b[1;32m    <a href='file:///home/yifu/.local/lib/python3.8/site-packages/transformers/utils/import_utils.py?line=785'>786</a>\u001b[0m \u001b[39mif\u001b[39;00m key\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39m_\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    <a href='file:///home/yifu/.local/lib/python3.8/site-packages/transformers/utils/import_utils.py?line=786'>787</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__getattr__\u001b[39m(\u001b[39mcls\u001b[39m, key)\n\u001b[0;32m--> <a href='file:///home/yifu/.local/lib/python3.8/site-packages/transformers/utils/import_utils.py?line=787'>788</a>\u001b[0m requires_backends(\u001b[39mcls\u001b[39;49m, \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_backends)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/utils/import_utils.py:776\u001b[0m, in \u001b[0;36mrequires_backends\u001b[0;34m(obj, backends)\u001b[0m\n\u001b[1;32m    <a href='file:///home/yifu/.local/lib/python3.8/site-packages/transformers/utils/import_utils.py?line=773'>774</a>\u001b[0m failed \u001b[39m=\u001b[39m [msg\u001b[39m.\u001b[39mformat(name) \u001b[39mfor\u001b[39;00m available, msg \u001b[39min\u001b[39;00m checks \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m available()]\n\u001b[1;32m    <a href='file:///home/yifu/.local/lib/python3.8/site-packages/transformers/utils/import_utils.py?line=774'>775</a>\u001b[0m \u001b[39mif\u001b[39;00m failed:\n\u001b[0;32m--> <a href='file:///home/yifu/.local/lib/python3.8/site-packages/transformers/utils/import_utils.py?line=775'>776</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(failed))\n",
      "\u001b[0;31mImportError\u001b[0m: \nAutoModel requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\n"
     ]
    }
   ],
   "source": [
    "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.51"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Measure the average length of the context (title)\n",
    "np.mean([len(i.split()) for i in train_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/yifu/nlp_project_yas/original_bert.ipynb Cell 13'\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blinux4.cs.uchicago.edu/home/yifu/nlp_project_yas/original_bert.ipynb#ch0000013vscode-remote?line=14'>15</a>\u001b[0m     tk \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mbatch_encode_plus(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blinux4.cs.uchicago.edu/home/yifu/nlp_project_yas/original_bert.ipynb#ch0000013vscode-remote?line=15'>16</a>\u001b[0m         text\u001b[39m.\u001b[39mtolist(),\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blinux4.cs.uchicago.edu/home/yifu/nlp_project_yas/original_bert.ipynb#ch0000013vscode-remote?line=16'>17</a>\u001b[0m         max_length \u001b[39m=\u001b[39m max_len,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blinux4.cs.uchicago.edu/home/yifu/nlp_project_yas/original_bert.ipynb#ch0000013vscode-remote?line=17'>18</a>\u001b[0m         pad_to_max_length\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blinux4.cs.uchicago.edu/home/yifu/nlp_project_yas/original_bert.ipynb#ch0000013vscode-remote?line=18'>19</a>\u001b[0m         truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blinux4.cs.uchicago.edu/home/yifu/nlp_project_yas/original_bert.ipynb#ch0000013vscode-remote?line=19'>20</a>\u001b[0m         )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blinux4.cs.uchicago.edu/home/yifu/nlp_project_yas/original_bert.ipynb#ch0000013vscode-remote?line=21'>22</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m tk\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Blinux4.cs.uchicago.edu/home/yifu/nlp_project_yas/original_bert.ipynb#ch0000013vscode-remote?line=23'>24</a>\u001b[0m train_tk \u001b[39m=\u001b[39m tokenize(train_text, L)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blinux4.cs.uchicago.edu/home/yifu/nlp_project_yas/original_bert.ipynb#ch0000013vscode-remote?line=24'>25</a>\u001b[0m val_tk \u001b[39m=\u001b[39m tokenize(val_text, L)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blinux4.cs.uchicago.edu/home/yifu/nlp_project_yas/original_bert.ipynb#ch0000013vscode-remote?line=25'>26</a>\u001b[0m test_tk \u001b[39m=\u001b[39m tokenize(test_text, L)\n",
      "\u001b[1;32m/home/yifu/nlp_project_yas/original_bert.ipynb Cell 13'\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(text, max_len)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blinux4.cs.uchicago.edu/home/yifu/nlp_project_yas/original_bert.ipynb#ch0000013vscode-remote?line=4'>5</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtokenize\u001b[39m(text, max_len):\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blinux4.cs.uchicago.edu/home/yifu/nlp_project_yas/original_bert.ipynb#ch0000013vscode-remote?line=5'>6</a>\u001b[0m     \u001b[39m'''\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blinux4.cs.uchicago.edu/home/yifu/nlp_project_yas/original_bert.ipynb#ch0000013vscode-remote?line=6'>7</a>\u001b[0m \u001b[39m    A function to turn English text into a token by the max length\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Blinux4.cs.uchicago.edu/home/yifu/nlp_project_yas/original_bert.ipynb#ch0000013vscode-remote?line=7'>8</a>\u001b[0m \u001b[39m    Inputs: \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blinux4.cs.uchicago.edu/home/yifu/nlp_project_yas/original_bert.ipynb#ch0000013vscode-remote?line=11'>12</a>\u001b[0m \u001b[39m      tk: an BERT Encoding object to train the model\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blinux4.cs.uchicago.edu/home/yifu/nlp_project_yas/original_bert.ipynb#ch0000013vscode-remote?line=12'>13</a>\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Blinux4.cs.uchicago.edu/home/yifu/nlp_project_yas/original_bert.ipynb#ch0000013vscode-remote?line=14'>15</a>\u001b[0m     tk \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mbatch_encode_plus(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blinux4.cs.uchicago.edu/home/yifu/nlp_project_yas/original_bert.ipynb#ch0000013vscode-remote?line=15'>16</a>\u001b[0m         text\u001b[39m.\u001b[39mtolist(),\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blinux4.cs.uchicago.edu/home/yifu/nlp_project_yas/original_bert.ipynb#ch0000013vscode-remote?line=16'>17</a>\u001b[0m         max_length \u001b[39m=\u001b[39m max_len,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blinux4.cs.uchicago.edu/home/yifu/nlp_project_yas/original_bert.ipynb#ch0000013vscode-remote?line=17'>18</a>\u001b[0m         pad_to_max_length\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blinux4.cs.uchicago.edu/home/yifu/nlp_project_yas/original_bert.ipynb#ch0000013vscode-remote?line=18'>19</a>\u001b[0m         truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blinux4.cs.uchicago.edu/home/yifu/nlp_project_yas/original_bert.ipynb#ch0000013vscode-remote?line=19'>20</a>\u001b[0m         )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blinux4.cs.uchicago.edu/home/yifu/nlp_project_yas/original_bert.ipynb#ch0000013vscode-remote?line=21'>22</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m tk\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "# Now that we have the average length of our sample, we tokenize them into an object for BERT to learn\n",
    "\n",
    "L = 15\n",
    "\n",
    "def tokenize(text, max_len):\n",
    "    '''\n",
    "    A function to turn English text into a token by the max length\n",
    "    Inputs: \n",
    "      text (str): the text to process\n",
    "      max_len (int): the maximum length of a token\n",
    "    Returns:\n",
    "      tk: an BERT Encoding object to train the model\n",
    "    '''\n",
    "\n",
    "    tk = tokenizer.batch_encode_plus(\n",
    "        text.tolist(),\n",
    "        max_length = max_len,\n",
    "        pad_to_max_length=True,\n",
    "        truncation=True\n",
    "        )\n",
    "\n",
    "    return tk\n",
    "\n",
    "train_tk = tokenize(train_text, L)\n",
    "val_tk = tokenize(val_text, L)\n",
    "test_tk = tokenize(test_text, L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizing every sentense, create a quantified database for machine learning\n",
    "\n",
    "train_seq = torch.tensor(train_tk['input_ids'])\n",
    "train_mask = torch.tensor(train_tk['attention_mask'])\n",
    "train_y = torch.tensor(train_labels.tolist())\n",
    "\n",
    "val_seq = torch.tensor(val_tk['input_ids'])\n",
    "val_mask = torch.tensor(val_tk['attention_mask'])\n",
    "val_y = torch.tensor(val_labels.tolist())\n",
    "\n",
    "test_seq = torch.tensor(test_tk['input_ids'])\n",
    "test_mask = torch.tensor(test_tk['attention_mask'])\n",
    "test_y = torch.tensor(test_labels.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An intuition: what we are doing: Vectorizing every sentense\n",
    "#print(train_text[0])\n",
    "#print(train_tk['input_ids'][0])\n",
    "#print(train_tk['attention_mask'][0])\n",
    "#print(train_labels.tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert\n",
    "# In each epoch of training, the model randomly select training datasets\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_data = TensorDataset(train_seq, train_mask, train_y)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "val_data = TensorDataset(val_seq, val_mask, val_y)\n",
    "val_sampler = SequentialSampler(val_data)\n",
    "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in bert.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "class BERT_Arch(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, bert):\n",
    "      \n",
    "        super(BERT_Arch, self).__init__()\n",
    "\n",
    "        self.bert = bert \n",
    "        self.dropout = torch.nn.Dropout(0.1)\n",
    "        self.relu =  torch.nn.ReLU()\n",
    "        self.fc1 = torch.nn.Linear(768,512)\n",
    "        self.fc2 = torch.nn.Linear(512,2)\n",
    "        self.softmax = torch.nn.LogSoftmax(dim=1)\n",
    "\n",
    "    #define the forward pass\n",
    "    def forward(self, sent_id, mask):\n",
    "        cls_hs = self.bert(sent_id, attention_mask=mask)['pooler_output']\n",
    "        x = self.fc1(cls_hs)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yifu/.local/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = BERT_Arch(bert)\n",
    "\n",
    "# define the optimizer\n",
    "optimizer = AdamW(model.parameters(),lr = 1e-5) # learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Part3: Training$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights: [0.93582888 1.07361963]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "class_weights = compute_class_weight(\n",
    "                                        class_weight = \"balanced\",\n",
    "                                        classes = np.unique(train_labels),\n",
    "                                        y = train_labels                                                    \n",
    "                                    )\n",
    "print(\"Class Weights:\",class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights= torch.tensor(class_weights,dtype=torch.float)\n",
    "\n",
    "cross_entropy  = torch.nn.NLLLoss(weight=weights) \n",
    "\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "  \n",
    "    model.train()\n",
    "\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "    \n",
    "    # empty list to save model predictions\n",
    "    total_preds=[]\n",
    "    \n",
    "    # iterate over batches\n",
    "    for step,batch in enumerate(train_dataloader):\n",
    "    \n",
    "        # progress update after every 50 batches.\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
    "        batch = [r for r in batch]\n",
    "        sent_id, mask, labels = batch\n",
    "        model.zero_grad()        \n",
    "        preds = model(sent_id, mask)\n",
    "        loss = cross_entropy(preds, labels)\n",
    "        total_loss = total_loss + loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        preds=preds.detach().cpu().numpy()\n",
    "        total_preds.append(preds)\n",
    "\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "    total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "    return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    \n",
    "    print(\"\\nEvaluating...\")\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "    total_preds = []\n",
    "    for step,batch in enumerate(val_dataloader):\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n",
    "\n",
    "        batch = [t for t in batch]\n",
    "\n",
    "        sent_id, mask, labels = batch\n",
    "\n",
    "        with torch.no_grad():\n",
    "            preds = model(sent_id, mask)\n",
    "            loss = cross_entropy(preds,labels)\n",
    "            total_loss = total_loss + loss.item()\n",
    "            preds = preds.detach().cpu().numpy()\n",
    "            total_preds.append(preds)\n",
    "\n",
    "    avg_loss = total_loss / len(val_dataloader) \n",
    "\n",
    "    # reshape the predictions in form of (number of samples, no. of classes)\n",
    "    total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "    return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1 / 10\n",
      "\n",
      "Evaluating...\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 122] Disk quota exceeded",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m/home/yifu/nlp_project_yas/original_bert.ipynb Cell 24'\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blinux4.cs.uchicago.edu/home/yifu/nlp_project_yas/original_bert.ipynb#ch0000023vscode-remote?line=18'>19</a>\u001b[0m \u001b[39mif\u001b[39;00m valid_loss \u001b[39m<\u001b[39m best_valid_loss:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blinux4.cs.uchicago.edu/home/yifu/nlp_project_yas/original_bert.ipynb#ch0000023vscode-remote?line=19'>20</a>\u001b[0m     best_valid_loss \u001b[39m=\u001b[39m valid_loss\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Blinux4.cs.uchicago.edu/home/yifu/nlp_project_yas/original_bert.ipynb#ch0000023vscode-remote?line=20'>21</a>\u001b[0m     torch\u001b[39m.\u001b[39;49msave(model\u001b[39m.\u001b[39;49mstate_dict(), \u001b[39m'\u001b[39;49m\u001b[39msaved_weights.pt\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blinux4.cs.uchicago.edu/home/yifu/nlp_project_yas/original_bert.ipynb#ch0000023vscode-remote?line=22'>23</a>\u001b[0m \u001b[39m# append training and validation loss\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Blinux4.cs.uchicago.edu/home/yifu/nlp_project_yas/original_bert.ipynb#ch0000023vscode-remote?line=23'>24</a>\u001b[0m train_losses\u001b[39m.\u001b[39mappend(train_loss)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/serialization.py:381\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    <a href='file:///home/yifu/.local/lib/python3.8/site-packages/torch/serialization.py?line=378'>379</a>\u001b[0m     \u001b[39mwith\u001b[39;00m _open_zipfile_writer(opened_file) \u001b[39mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m    <a href='file:///home/yifu/.local/lib/python3.8/site-packages/torch/serialization.py?line=379'>380</a>\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol)\n\u001b[0;32m--> <a href='file:///home/yifu/.local/lib/python3.8/site-packages/torch/serialization.py?line=380'>381</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/yifu/.local/lib/python3.8/site-packages/torch/serialization.py?line=381'>382</a>\u001b[0m _legacy_save(obj, opened_file, pickle_module, pickle_protocol)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/serialization.py:215\u001b[0m, in \u001b[0;36m_open_file.__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    <a href='file:///home/yifu/.local/lib/python3.8/site-packages/torch/serialization.py?line=213'>214</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__exit__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs):\n\u001b[0;32m--> <a href='file:///home/yifu/.local/lib/python3.8/site-packages/torch/serialization.py?line=214'>215</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfile_like\u001b[39m.\u001b[39;49mclose()\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 122] Disk quota exceeded"
     ]
    }
   ],
   "source": [
    "best_valid_loss = float('inf')\n",
    "\n",
    "# empty lists to store training and validation loss of each epoch\n",
    "train_losses=[]\n",
    "valid_losses=[]\n",
    "\n",
    "#for each epoch\n",
    "for epoch in range(epochs):\n",
    "     \n",
    "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
    "\n",
    "    train_loss, _ = train()\n",
    "\n",
    "    valid_loss, _ = evaluate()\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    print(f'\\nTraining Loss: {train_loss:.3f}')\n",
    "    print(f'Validation Loss: {valid_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load weights of best model\n",
    "path = 'saved_weights.pt'\n",
    "model.load_state_dict(torch.load(path))\n",
    "with torch.no_grad():\n",
    "    preds = model(test_seq, test_mask)\n",
    "    preds = preds.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Part4: Performance$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check precision, recall and f1-score \n",
    "preds = np.argmax(preds, axis = 1)\n",
    "print(classification_report(test_y, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute a confusion maatrix on prediction results \n",
    "confusion_matrix(preds,test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
