{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/dk/pln7xlc14nzbppm5dwwxnl1h0000gn/T/ipykernel_17979/1519813024.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "#import spacy\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data cleaning\n",
    "\n",
    "# Import data, add target classification, merge dataset, add label\n",
    "fake = pd.read_csv(\"data/fake.csv\", usecols= [\"text\"])\n",
    "true = pd.read_csv(\"data/true.csv\", usecols= [\"text\"])\n",
    "fake['target'] = 0\n",
    "true['target'] = 1\n",
    "\n",
    "#remove newspaper source (Reuters) from the \"true\" articles before merging\n",
    "true['text'] = true['text'].replace(r'\\A.*\\(Reuters\\)', '', regex=True) \n",
    "\n",
    "# Merge fake and true articles into one dataset \n",
    "data = true.append(fake).sample(frac=1).reset_index().drop(columns=['index'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of the dataset `data` consisting of both labels(1 is True, 0 is Fake):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Robert Parry Consortium NewsIf there were any...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>While Bernie Sanders is nowhere near ready to ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>- Life has stopped in its tracks in Myanmar s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>On Tuesday night, Ted Cruz dropped out of the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>- Britain is very close to reaching an agreem...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44893</th>\n",
       "      <td>If you ve spent much time on social media, you...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44894</th>\n",
       "      <td>- The number of asylum seekers walking across...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44895</th>\n",
       "      <td>- Turkey s talks with the United States over ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44896</th>\n",
       "      <td>Tune in to the Alternate Current Radio Network...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44897</th>\n",
       "      <td>- Thousands of Romanians lined the streets of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44898 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  target\n",
       "0       Robert Parry Consortium NewsIf there were any...       0\n",
       "1      While Bernie Sanders is nowhere near ready to ...       0\n",
       "2       - Life has stopped in its tracks in Myanmar s...       1\n",
       "3      On Tuesday night, Ted Cruz dropped out of the ...       0\n",
       "4       - Britain is very close to reaching an agreem...       1\n",
       "...                                                  ...     ...\n",
       "44893  If you ve spent much time on social media, you...       0\n",
       "44894   - The number of asylum seekers walking across...       1\n",
       "44895   - Turkey s talks with the United States over ...       1\n",
       "44896  Tune in to the Alternate Current Radio Network...       0\n",
       "44897   - Thousands of Romanians lined the streets of...       1\n",
       "\n",
       "[44898 rows x 2 columns]"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into two parts: training data (7/10) and other data (3/10)\n",
    "train_text, val_test_text = train_test_split(data, random_state=1234, test_size=0.3, stratify=data['target'])\n",
    "\n",
    "# Split other data into two parts: validation data (1/3 * 3/10 = 1/10) and testing data (2/3 * 3/10 = 2/10)\n",
    "val_text, test_text = train_test_split(val_test_text, random_state=1234, test_size=0.6, stratify=val_test_text['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of the `train_text` data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22256</th>\n",
       "      <td>- When more than 7.4 million homes and busine...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29536</th>\n",
       "      <td>Yes, he did say that bad grammar and all! Can ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15851</th>\n",
       "      <td>Rachel Maddow decided to scrap a segment and g...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21257</th>\n",
       "      <td>- U.S.-backed Syrian militias will not let go...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12532</th>\n",
       "      <td>- Support for the creation of an independent ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10984</th>\n",
       "      <td>It s just cleaner that way You know, keeping a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2934</th>\n",
       "      <td>Why would the Vatican invite an aggressive lef...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40321</th>\n",
       "      <td>- Polish Prime Minister Beata Szydlo said on ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33504</th>\n",
       "      <td>Washington was rocked by yet another staff sha...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3201</th>\n",
       "      <td>- German prosecutors have launched an investi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31428 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  target\n",
       "22256   - When more than 7.4 million homes and busine...       1\n",
       "29536  Yes, he did say that bad grammar and all! Can ...       0\n",
       "15851  Rachel Maddow decided to scrap a segment and g...       0\n",
       "21257   - U.S.-backed Syrian militias will not let go...       1\n",
       "12532   - Support for the creation of an independent ...       1\n",
       "...                                                  ...     ...\n",
       "10984  It s just cleaner that way You know, keeping a...       0\n",
       "2934   Why would the Vatican invite an aggressive lef...       0\n",
       "40321   - Polish Prime Minister Beata Szydlo said on ...       1\n",
       "33504  Washington was rocked by yet another staff sha...       0\n",
       "3201    - German prosecutors have launched an investi...       1\n",
       "\n",
       "[31428 rows x 2 columns]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our baseline model, we will be using the `TF-IDF` Vectorizer to pre-process articles and then apply Logistic Classifier.\n",
    "\n",
    "- **fit_transform()** method learns vocabulary and `IDF` used for both training & test data. Returns document-term matrix with calculated `TF-IDF` values.\n",
    "\n",
    "- **transform()** method uses the vocabulary and document frequencies (df) learned by **fit_transform()**. Returns document-term matrix with calculated `TF-IDF` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note, ngrams = 1, which is the default value if not specified in TfidfVectorizer. \n",
    "text_transformer = TfidfVectorizer(stop_words='english', lowercase=True, max_features=1000)\n",
    "\n",
    "# vectorize train and test data. Produce TF-IDF for train data\n",
    "X_train_text = text_transformer.fit_transform(train_text['text'])\n",
    "X_val_text = text_transformer.transform(val_text['text'])\n",
    "X_test_text = text_transformer.transform(test_text['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the example of the stop words used in TfidfVectorizer that will be filtered out from our observations (i.e. articles), both 'training' and 'test':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "frozenset({'a',\n",
       "           'about',\n",
       "           'above',\n",
       "           'across',\n",
       "           'after',\n",
       "           'afterwards',\n",
       "           'again',\n",
       "           'against',\n",
       "           'all',\n",
       "           'almost',\n",
       "           'alone',\n",
       "           'along',\n",
       "           'already',\n",
       "           'also',\n",
       "           'although',\n",
       "           'always',\n",
       "           'am',\n",
       "           'among',\n",
       "           'amongst',\n",
       "           'amoungst',\n",
       "           'amount',\n",
       "           'an',\n",
       "           'and',\n",
       "           'another',\n",
       "           'any',\n",
       "           'anyhow',\n",
       "           'anyone',\n",
       "           'anything',\n",
       "           'anyway',\n",
       "           'anywhere',\n",
       "           'are',\n",
       "           'around',\n",
       "           'as',\n",
       "           'at',\n",
       "           'back',\n",
       "           'be',\n",
       "           'became',\n",
       "           'because',\n",
       "           'become',\n",
       "           'becomes',\n",
       "           'becoming',\n",
       "           'been',\n",
       "           'before',\n",
       "           'beforehand',\n",
       "           'behind',\n",
       "           'being',\n",
       "           'below',\n",
       "           'beside',\n",
       "           'besides',\n",
       "           'between',\n",
       "           'beyond',\n",
       "           'bill',\n",
       "           'both',\n",
       "           'bottom',\n",
       "           'but',\n",
       "           'by',\n",
       "           'call',\n",
       "           'can',\n",
       "           'cannot',\n",
       "           'cant',\n",
       "           'co',\n",
       "           'con',\n",
       "           'could',\n",
       "           'couldnt',\n",
       "           'cry',\n",
       "           'de',\n",
       "           'describe',\n",
       "           'detail',\n",
       "           'do',\n",
       "           'done',\n",
       "           'down',\n",
       "           'due',\n",
       "           'during',\n",
       "           'each',\n",
       "           'eg',\n",
       "           'eight',\n",
       "           'either',\n",
       "           'eleven',\n",
       "           'else',\n",
       "           'elsewhere',\n",
       "           'empty',\n",
       "           'enough',\n",
       "           'etc',\n",
       "           'even',\n",
       "           'ever',\n",
       "           'every',\n",
       "           'everyone',\n",
       "           'everything',\n",
       "           'everywhere',\n",
       "           'except',\n",
       "           'few',\n",
       "           'fifteen',\n",
       "           'fifty',\n",
       "           'fill',\n",
       "           'find',\n",
       "           'fire',\n",
       "           'first',\n",
       "           'five',\n",
       "           'for',\n",
       "           'former',\n",
       "           'formerly',\n",
       "           'forty',\n",
       "           'found',\n",
       "           'four',\n",
       "           'from',\n",
       "           'front',\n",
       "           'full',\n",
       "           'further',\n",
       "           'get',\n",
       "           'give',\n",
       "           'go',\n",
       "           'had',\n",
       "           'has',\n",
       "           'hasnt',\n",
       "           'have',\n",
       "           'he',\n",
       "           'hence',\n",
       "           'her',\n",
       "           'here',\n",
       "           'hereafter',\n",
       "           'hereby',\n",
       "           'herein',\n",
       "           'hereupon',\n",
       "           'hers',\n",
       "           'herself',\n",
       "           'him',\n",
       "           'himself',\n",
       "           'his',\n",
       "           'how',\n",
       "           'however',\n",
       "           'hundred',\n",
       "           'i',\n",
       "           'ie',\n",
       "           'if',\n",
       "           'in',\n",
       "           'inc',\n",
       "           'indeed',\n",
       "           'interest',\n",
       "           'into',\n",
       "           'is',\n",
       "           'it',\n",
       "           'its',\n",
       "           'itself',\n",
       "           'keep',\n",
       "           'last',\n",
       "           'latter',\n",
       "           'latterly',\n",
       "           'least',\n",
       "           'less',\n",
       "           'ltd',\n",
       "           'made',\n",
       "           'many',\n",
       "           'may',\n",
       "           'me',\n",
       "           'meanwhile',\n",
       "           'might',\n",
       "           'mill',\n",
       "           'mine',\n",
       "           'more',\n",
       "           'moreover',\n",
       "           'most',\n",
       "           'mostly',\n",
       "           'move',\n",
       "           'much',\n",
       "           'must',\n",
       "           'my',\n",
       "           'myself',\n",
       "           'name',\n",
       "           'namely',\n",
       "           'neither',\n",
       "           'never',\n",
       "           'nevertheless',\n",
       "           'next',\n",
       "           'nine',\n",
       "           'no',\n",
       "           'nobody',\n",
       "           'none',\n",
       "           'noone',\n",
       "           'nor',\n",
       "           'not',\n",
       "           'nothing',\n",
       "           'now',\n",
       "           'nowhere',\n",
       "           'of',\n",
       "           'off',\n",
       "           'often',\n",
       "           'on',\n",
       "           'once',\n",
       "           'one',\n",
       "           'only',\n",
       "           'onto',\n",
       "           'or',\n",
       "           'other',\n",
       "           'others',\n",
       "           'otherwise',\n",
       "           'our',\n",
       "           'ours',\n",
       "           'ourselves',\n",
       "           'out',\n",
       "           'over',\n",
       "           'own',\n",
       "           'part',\n",
       "           'per',\n",
       "           'perhaps',\n",
       "           'please',\n",
       "           'put',\n",
       "           'rather',\n",
       "           're',\n",
       "           'same',\n",
       "           'see',\n",
       "           'seem',\n",
       "           'seemed',\n",
       "           'seeming',\n",
       "           'seems',\n",
       "           'serious',\n",
       "           'several',\n",
       "           'she',\n",
       "           'should',\n",
       "           'show',\n",
       "           'side',\n",
       "           'since',\n",
       "           'sincere',\n",
       "           'six',\n",
       "           'sixty',\n",
       "           'so',\n",
       "           'some',\n",
       "           'somehow',\n",
       "           'someone',\n",
       "           'something',\n",
       "           'sometime',\n",
       "           'sometimes',\n",
       "           'somewhere',\n",
       "           'still',\n",
       "           'such',\n",
       "           'system',\n",
       "           'take',\n",
       "           'ten',\n",
       "           'than',\n",
       "           'that',\n",
       "           'the',\n",
       "           'their',\n",
       "           'them',\n",
       "           'themselves',\n",
       "           'then',\n",
       "           'thence',\n",
       "           'there',\n",
       "           'thereafter',\n",
       "           'thereby',\n",
       "           'therefore',\n",
       "           'therein',\n",
       "           'thereupon',\n",
       "           'these',\n",
       "           'they',\n",
       "           'thick',\n",
       "           'thin',\n",
       "           'third',\n",
       "           'this',\n",
       "           'those',\n",
       "           'though',\n",
       "           'three',\n",
       "           'through',\n",
       "           'throughout',\n",
       "           'thru',\n",
       "           'thus',\n",
       "           'to',\n",
       "           'together',\n",
       "           'too',\n",
       "           'top',\n",
       "           'toward',\n",
       "           'towards',\n",
       "           'twelve',\n",
       "           'twenty',\n",
       "           'two',\n",
       "           'un',\n",
       "           'under',\n",
       "           'until',\n",
       "           'up',\n",
       "           'upon',\n",
       "           'us',\n",
       "           'very',\n",
       "           'via',\n",
       "           'was',\n",
       "           'we',\n",
       "           'well',\n",
       "           'were',\n",
       "           'what',\n",
       "           'whatever',\n",
       "           'when',\n",
       "           'whence',\n",
       "           'whenever',\n",
       "           'where',\n",
       "           'whereafter',\n",
       "           'whereas',\n",
       "           'whereby',\n",
       "           'wherein',\n",
       "           'whereupon',\n",
       "           'wherever',\n",
       "           'whether',\n",
       "           'which',\n",
       "           'while',\n",
       "           'whither',\n",
       "           'who',\n",
       "           'whoever',\n",
       "           'whole',\n",
       "           'whom',\n",
       "           'whose',\n",
       "           'why',\n",
       "           'will',\n",
       "           'with',\n",
       "           'within',\n",
       "           'without',\n",
       "           'would',\n",
       "           'yet',\n",
       "           'you',\n",
       "           'your',\n",
       "           'yours',\n",
       "           'yourself',\n",
       "           'yourselves'})"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_transformer.get_stop_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['000', '10', '100', '11', '12', '13', '14', '15', '16', '17', '18',\n",
       "       '19', '20', '2008', '2011', '2012', '2013', '2014', '2015', '2016',\n",
       "       '2017', '21st', '21wire', '22', '24', '25', '28', '30', '50',\n",
       "       'able', 'abortion', 'absolutely', 'access', 'according', 'account',\n",
       "       'accused', 'act', 'action', 'actions', 'actually', 'added',\n",
       "       'adding', 'address', 'administration', 'adviser', 'afghanistan',\n",
       "       'african', 'agencies', 'agency', 'agenda', 'agents', 'ago',\n",
       "       'agreed', 'agreement', 'ahead', 'aid', 'air', 'al', 'allegations',\n",
       "       'alleged', 'allies', 'allow', 'allowed', 'ambassador', 'amendment',\n",
       "       'america', 'american', 'americans', 'announced', 'answer', 'anti',\n",
       "       'apparently', 'appeared', 'appears', 'april', 'arabia', 'area',\n",
       "       'areas', 'armed', 'army', 'arrested', 'article', 'ask', 'asked',\n",
       "       'asking', 'assault', 'attack', 'attacks', 'attempt', 'attention',\n",
       "       'attorney', 'august', 'authorities', 'authority', 'away', 'backed',\n",
       "       'bad', 'ban', 'bank', 'barack', 'base', 'based', 'began',\n",
       "       'behavior', 'believe', 'bernie', 'best', 'better', 'big',\n",
       "       'biggest', 'billion', 'black', 'board', 'body', 'book', 'border',\n",
       "       'breitbart', 'brexit', 'bring', 'britain', 'british', 'brought',\n",
       "       'budget', 'build', 'building', 'bush', 'business', 'businesses',\n",
       "       'california', 'called', 'calling', 'calls', 'came', 'campaign',\n",
       "       'candidate', 'candidates', 'capital', 'care', 'carolina', 'case',\n",
       "       'cases', 'cause', 'center', 'central', 'century', 'certain',\n",
       "       'certainly', 'chairman', 'chance', 'change', 'changes', 'charge',\n",
       "       'charged', 'charges', 'chief', 'child', 'children', 'china',\n",
       "       'chinese', 'choice', 'christian', 'church', 'cia', 'cities',\n",
       "       'citizens', 'city', 'civil', 'claim', 'claimed', 'claims', 'class',\n",
       "       'clear', 'clearly', 'climate', 'clinton', 'close', 'cnn',\n",
       "       'coalition', 'college', 'com', 'come', 'comes', 'comey', 'coming',\n",
       "       'comment', 'comments', 'commission', 'committed', 'committee',\n",
       "       'community', 'companies', 'company', 'completely', 'concern',\n",
       "       'concerned', 'concerns', 'conference', 'confirmed', 'conflict',\n",
       "       'congress', 'congressional', 'conservative', 'conservatives',\n",
       "       'consider', 'considered', 'constitution', 'continue', 'continued',\n",
       "       'control', 'convention', 'corruption', 'cost', 'council',\n",
       "       'countries', 'country', 'county', 'course', 'court', 'cover',\n",
       "       'coverage', 'create', 'created', 'crime', 'crimes', 'criminal',\n",
       "       'crisis', 'critical', 'criticism', 'criticized', 'crowd', 'cruz',\n",
       "       'current', 'currently', 'cut', 'daily', 'dangerous', 'data',\n",
       "       'david', 'day', 'days', 'dead', 'deal', 'death', 'debate', 'debt',\n",
       "       'decades', 'december', 'decided', 'decision', 'declined',\n",
       "       'defense', 'democracy', 'democrat', 'democratic', 'democrats',\n",
       "       'denied', 'department', 'deputy', 'described', 'despite',\n",
       "       'details', 'development', 'did', 'didn', 'died', 'different',\n",
       "       'diplomatic', 'direct', 'director', 'discuss', 'district',\n",
       "       'documents', 'does', 'doesn', 'doing', 'dollars', 'don', 'donald',\n",
       "       'drug', 'earlier', 'early', 'east', 'economic', 'economy',\n",
       "       'education', 'effort', 'efforts', 'elect', 'elected', 'election',\n",
       "       'elections', 'email', 'emails', 'employees', 'end', 'energy',\n",
       "       'enforcement', 'ensure', 'entire', 'especially', 'establishment',\n",
       "       'eu', 'europe', 'european', 'event', 'events', 'evidence',\n",
       "       'exactly', 'example', 'executive', 'expected', 'experts',\n",
       "       'expressed', 'face', 'facebook', 'fact', 'failed', 'fake', 'false',\n",
       "       'families', 'family', 'far', 'father', 'favor', 'fbi', 'fear',\n",
       "       'featured', 'february', 'federal', 'feel', 'fellow', 'fight',\n",
       "       'fighting', 'filed', 'final', 'financial', 'fired', 'firm', 'flag',\n",
       "       'florida', 'flynn', 'focus', 'following', 'food', 'force',\n",
       "       'forced', 'forces', 'foreign', 'form', 'forward', 'foundation',\n",
       "       'fox', 'france', 'fraud', 'free', 'freedom', 'french', 'friday',\n",
       "       'friends', 'fund', 'funding', 'funds', 'future', 'gave', 'gay',\n",
       "       'general', 'george', 'german', 'germany', 'getting', 'getty',\n",
       "       'given', 'giving', 'global', 'god', 'goes', 'going', 'good', 'gop',\n",
       "       'got', 'government', 'governor', 'great', 'ground', 'group',\n",
       "       'groups', 'growing', 'gun', 'guy', 'half', 'hand', 'hands',\n",
       "       'happen', 'happened', 'hard', 'hate', 'having', 'head', 'health',\n",
       "       'healthcare', 'hear', 'heard', 'hearing', 'held', 'help', 'helped',\n",
       "       'high', 'hillary', 'history', 'hit', 'hold', 'home', 'hope',\n",
       "       'host', 'hours', 'house', 'https', 'huge', 'human', 'hundreds',\n",
       "       'husband', 'idea', 'illegal', 'image', 'images', 'immediately',\n",
       "       'immigrants', 'immigration', 'important', 'incident', 'include',\n",
       "       'included', 'including', 'income', 'increase', 'independence',\n",
       "       'independent', 'individuals', 'industry', 'influence',\n",
       "       'information', 'inside', 'instead', 'insurance', 'intelligence',\n",
       "       'interests', 'international', 'internet', 'interview',\n",
       "       'investigation', 'involved', 'iran', 'iraq', 'iraqi', 'isis',\n",
       "       'islamic', 'island', 'isn', 'israel', 'israeli', 'issue', 'issued',\n",
       "       'issues', 'james', 'january', 'japan', 'job', 'jobs', 'joe',\n",
       "       'john', 'johnson', 'judge', 'july', 'june', 'just', 'justice',\n",
       "       'kelly', 'key', 'killed', 'killing', 'kind', 'king', 'knew',\n",
       "       'know', 'known', 'knows', 'korea', 'korean', 'kurdish', 'land',\n",
       "       'large', 'largest', 'late', 'later', 'latest', 'law', 'lawmakers',\n",
       "       'laws', 'lawyer', 'lead', 'leader', 'leaders', 'leadership',\n",
       "       'leading', 'leave', 'leaving', 'led', 'left', 'legal',\n",
       "       'legislation', 'let', 'letter', 'level', 'liberal', 'life', 'like',\n",
       "       'likely', 'line', 'list', 'little', 'live', 'lives', 'living',\n",
       "       'll', 'local', 'long', 'longer', 'look', 'looking', 'lost', 'lot',\n",
       "       'love', 'low', 'lower', 'main', 'mainstream', 'major', 'majority',\n",
       "       'make', 'makes', 'making', 'man', 'march', 'mark', 'market',\n",
       "       'mass', 'matter', 'mayor', 'mccain', 'mcconnell', 'mean', 'means',\n",
       "       'measure', 'measures', 'media', 'medical', 'meet', 'meeting',\n",
       "       'member', 'members', 'men', 'merkel', 'message', 'met', 'mexico',\n",
       "       'michael', 'middle', 'mike', 'militants', 'military', 'million',\n",
       "       'millions', 'minister', 'ministry', 'missile', 'moment', 'monday',\n",
       "       'money', 'month', 'months', 'moore', 'morning', 'moscow', 'mother',\n",
       "       'movement', 'mr', 'muslim', 'muslims', 'myanmar', 'named',\n",
       "       'nation', 'national', 'nations', 'nato', 'near', 'nearly', 'need',\n",
       "       'needed', 'needs', 'network', 'new', 'news', 'night', 'nomination',\n",
       "       'nominee', 'non', 'north', 'northern', 'nov', 'november',\n",
       "       'nuclear', 'number', 'numbers', 'obama', 'obamacare', 'october',\n",
       "       'office', 'officer', 'officers', 'official', 'officials', 'oil',\n",
       "       'old', 'online', 'open', 'operations', 'opinion', 'opposition',\n",
       "       'order', 'organization', 'organizations', 'outside', 'page',\n",
       "       'paid', 'panel', 'parliament', 'parties', 'party', 'pass',\n",
       "       'passed', 'past', 'paul', 'pay', 'peace', 'pence', 'people',\n",
       "       'percent', 'person', 'personal', 'phone', 'photo', 'pic', 'place',\n",
       "       'plan', 'planned', 'plans', 'play', 'point', 'points', 'police',\n",
       "       'policies', 'policy', 'political', 'politicians', 'politics',\n",
       "       'poll', 'polls', 'poor', 'popular', 'position', 'possible', 'post',\n",
       "       'posted', 'potential', 'power', 'powerful', 'presidency',\n",
       "       'president', 'presidential', 'press', 'pressure', 'pretty',\n",
       "       'prevent', 'previous', 'previously', 'primary', 'prime', 'prison',\n",
       "       'private', 'pro', 'probably', 'problem', 'problems', 'process',\n",
       "       'program', 'programs', 'progress', 'project', 'proposal',\n",
       "       'proposed', 'protect', 'protest', 'protesters', 'protests',\n",
       "       'provide', 'provided', 'public', 'published', 'push', 'putin',\n",
       "       'question', 'questions', 'quickly', 'race', 'racist', 'radio',\n",
       "       'raised', 'rally', 'rate', 'read', 'ready', 'real',\n",
       "       'realdonaldtrump', 'reality', 'really', 'reason', 'received',\n",
       "       'recent', 'recently', 'record', 'records', 'reform', 'refugee',\n",
       "       'refugees', 'refused', 'region', 'regional', 'related',\n",
       "       'relations', 'relationship', 'release', 'released', 'religious',\n",
       "       'remain', 'remarks', 'rep', 'repeatedly', 'report', 'reported',\n",
       "       'reporter', 'reporters', 'reports', 'representative',\n",
       "       'representatives', 'republican', 'republicans', 'request',\n",
       "       'research', 'residents', 'respect', 'respond', 'responded',\n",
       "       'response', 'responsible', 'result', 'results', 'return',\n",
       "       'reuters', 'review', 'rich', 'right', 'rights', 'risk', 'robert',\n",
       "       'rohingya', 'role', 'room', 'rubio', 'rule', 'rules', 'ruling',\n",
       "       'run', 'running', 'russia', 'russian', 'ryan', 'safe', 'safety',\n",
       "       'said', 'san', 'sanctions', 'sanders', 'saturday', 'saudi', 'saw',\n",
       "       'say', 'saying', 'says', 'school', 'second', 'secret', 'secretary',\n",
       "       'security', 'seek', 'seeking', 'seen', 'self', 'senate', 'senator',\n",
       "       'senators', 'senior', 'sent', 'september', 'series', 'served',\n",
       "       'service', 'services', 'sessions', 'set', 'seven', 'sex', 'sexual',\n",
       "       'share', 'shooting', 'short', 'shot', 'showed', 'shows', 'sign',\n",
       "       'signed', 'similar', 'simply', 'single', 'situation', 'small',\n",
       "       'social', 'society', 'son', 'soon', 'source', 'sources', 'south',\n",
       "       'southern', 'speak', 'speaker', 'speaking', 'special', 'speech',\n",
       "       'spending', 'spent', 'spoke', 'spokesman', 'spokeswoman', 'staff',\n",
       "       'stand', 'standing', 'star', 'start', 'started', 'state',\n",
       "       'statement', 'states', 'status', 'stay', 'step', 'stop', 'story',\n",
       "       'street', 'strong', 'student', 'students', 'sunday', 'support',\n",
       "       'supporters', 'supreme', 'sure', 'syria', 'syrian', 'taken',\n",
       "       'taking', 'talk', 'talking', 'talks', 'target', 'tax', 'taxes',\n",
       "       'team', 'ted', 'television', 'tell', 'telling', 'term', 'terms',\n",
       "       'terror', 'terrorism', 'terrorist', 'terrorists', 'texas', 'thing',\n",
       "       'things', 'think', 'thought', 'thousands', 'threat', 'threats',\n",
       "       'thursday', 'ties', 'tillerson', 'time', 'times', 'today', 'told',\n",
       "       'took', 'total', 'town', 'trade', 'transition', 'travel', 'tried',\n",
       "       'trip', 'troops', 'true', 'trump', 'truth', 'try', 'trying',\n",
       "       'tuesday', 'turkey', 'turn', 'turned', 'tv', 'tweet', 'tweeted',\n",
       "       'twitter', 'understand', 'union', 'united', 'university', 'urged',\n",
       "       'use', 'used', 'using', 've', 'vice', 'victims', 'victory',\n",
       "       'video', 'view', 'views', 'violence', 'violent', 'virginia',\n",
       "       'visit', 'vote', 'voted', 'voter', 'voters', 'votes', 'voting',\n",
       "       'wall', 'want', 'wanted', 'wants', 'war', 'warned', 'washington',\n",
       "       'wasn', 'watch', 'water', 'way', 'weapons', 'website', 'wednesday',\n",
       "       'week', 'weeks', 'went', 'west', 'western', 'white', 'wife', 'win',\n",
       "       'wing', 'wire', 'woman', 'women', 'won', 'words', 'work', 'worked',\n",
       "       'workers', 'working', 'world', 'wrong', 'wrote', 'year', 'years',\n",
       "       'yes', 'york', 'young', 'youtube'], dtype=object)"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = text_transformer.get_feature_names_out()\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of observations (articles) in  the test data:  31428\n",
      "The number of features (tokens) in  the test data:  1000\n"
     ]
    }
   ],
   "source": [
    "print('The number of observations (articles) in  the test data: ', X_train_text.shape[0])\n",
    "print('The number of features (tokens) in  the test data: ', X_train_text.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of `TF-IDF` matrix, **val_text**, for the validation dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        ...,\n",
       "        [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.02782728, 0.03085852, 0.        , ..., 0.        , 0.        ,\n",
       "         0.08138153],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        , 0.07077367,\n",
       "         0.        ]])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val_text.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the `Logistic Classifier` as our baseline model for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1, multi_class='multinomial', solver='sag')"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit = LogisticRegression(penalty = 'l2', C = 1, solver= 'sag', multi_class = 'multinomial')\n",
    "logit.fit(X_train_text, train_text['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have trained our model, we will apply it to predict labels (true/false) for articles in the test data and calculate the accuracy score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the accuracy score on the training data is:  0.9772814051164567\n",
      "the accuracy score on the validation data is:  0.9699331848552338\n"
     ]
    }
   ],
   "source": [
    "train_predicted_label = logit.predict(X_train_text)\n",
    "train_accuracy_score = accuracy_score(train_text['target'], train_predicted_label)\n",
    "\n",
    "predicted_label = logit.predict(X_val_text)\n",
    "accuracy_score = accuracy_score(val_text['target'], predicted_label)\n",
    "\n",
    "print('the accuracy score on the training data is: ', train_accuracy_score)\n",
    "print('the accuracy score on the validation data is: ', accuracy_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Future steps:**\n",
    "\n",
    "- Continue cleaning data with the use of Regex and other packages (digits, punctation, 'Router', '21st century')\n",
    "\n",
    "- Further analysis of data\n",
    "\n",
    "- Re-run model after data is cleaned\n",
    "\n",
    "- Discover options to improve the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes form meeting with Cole:\n",
    "\n",
    "try different models with different # number of features \n",
    "\n",
    "1. spacy for steming/punctation \n",
    "2. remove article sources \n",
    "3. try different # of features \n",
    "\n",
    "parced list of words\n",
    "\n",
    "analysis: size of trainings \n",
    "distribution of words "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d7660429b82a00d1826214792335f268fe35060b99a4b0365a6603395481d28c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
