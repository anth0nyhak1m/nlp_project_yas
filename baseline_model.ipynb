{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#importing helper functions for pre-processing data\n",
    "from util import cleaning_data, stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oleksandrafilippova/ML_class/final_project/nlp_project_yas/util.py:35: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  data = true_df.append(fake_df).sample(frac=1).reset_index().drop(columns=['index'])\n"
     ]
    }
   ],
   "source": [
    "# cleanine_data function loads in data, adds labels, and removes punctuation \n",
    "data = cleaning_data(\"data/true.csv\", \"data/fake.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of the dataset `data` consisting of both labels(1 is True, 0 is Fake):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>new us ambassador nikki haley strutted un frid...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dear mike tomlin james harrison ben roethlisbe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>syria opposition wants russia states put press...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>south africa revenue service ask parliament in...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>weak recent inflation readings worry suggest f...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44893</th>\n",
       "      <td>hurricane irma moving little west category hur...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44894</th>\n",
       "      <td>patrick henningsen 21st century wirewatching w...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44895</th>\n",
       "      <td>big name security state sovereignty tennessee ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44896</th>\n",
       "      <td>jason kander democrat running united states se...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44897</th>\n",
       "      <td>state department said tuesday providing additi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44898 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  target\n",
       "0      new us ambassador nikki haley strutted un frid...       0\n",
       "1      dear mike tomlin james harrison ben roethlisbe...       0\n",
       "2      syria opposition wants russia states put press...       1\n",
       "3      south africa revenue service ask parliament in...       1\n",
       "4      weak recent inflation readings worry suggest f...       1\n",
       "...                                                  ...     ...\n",
       "44893  hurricane irma moving little west category hur...       0\n",
       "44894  patrick henningsen 21st century wirewatching w...       0\n",
       "44895  big name security state sovereignty tennessee ...       0\n",
       "44896  jason kander democrat running united states se...       0\n",
       "44897  state department said tuesday providing additi...       1\n",
       "\n",
       "[44898 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distribution of word frequencies before stemming: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    121766.000000\n",
       "mean         87.001774\n",
       "std         879.965508\n",
       "min           1.000000\n",
       "25%           1.000000\n",
       "50%           2.000000\n",
       "75%          11.000000\n",
       "max      133992.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('distribution of word frequencies before stemming: ')\n",
    "pd.Series(' '.join(data['text']).split()).value_counts().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 10 words before stemming are: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "trump        133992\n",
       "said         132816\n",
       "president     55890\n",
       "would         55165\n",
       "people        41855\n",
       "one           37892\n",
       "state         34486\n",
       "also          31357\n",
       "new           30310\n",
       "clinton       28694\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('top 10 words before stemming are: ')\n",
    "pd.Series(' '.join(data['text']).split()).value_counts()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From the descriptive statistics above: \n",
    "\n",
    "- The dataset consists `121,766` unique words after stop words (i.e., *you*, *she*, *and*).\n",
    "- The median frequency among all words equals to `2`. Meaning, at least a half of all words were mentioned once or twice in the entire dataset.\n",
    "- The word that was mentioned the most is `Trump`. It was mentioned `133,992` times. \n",
    "\n",
    "Having `121,766` features for a `40,000`-observation dataset we are risking to encounter **Curse of Dimensionality**. We need to reduce the number of total features before training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = stemming(data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distribution of word frequencies after stemming: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count     95924.000000\n",
       "mean        110.440119\n",
       "std        1126.504906\n",
       "min           1.000000\n",
       "25%           1.000000\n",
       "50%           2.000000\n",
       "75%           8.000000\n",
       "max      134244.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('distribution of word frequencies after stemming: ')\n",
    "pd.Series(' '.join(data['text']).split()).value_counts().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 10 stems after stemming are: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "trump         134244\n",
       "said          132816\n",
       "state          63382\n",
       "presid         60429\n",
       "would          55165\n",
       "peopl          42011\n",
       "year           41759\n",
       "republican     39743\n",
       "one            39104\n",
       "say            36911\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('top 10 stems after stemming are: ')\n",
    "pd.Series(' '.join(data['text']).split()).value_counts()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into two parts: training data (7/10) and other data (3/10)\n",
    "train_text, val_test_text = train_test_split(data, random_state=1234, test_size=0.3, stratify=data['target'])\n",
    "\n",
    "# Split other data into two parts: validation data (1/3 * 3/10 = 1/10) and testing data (2/3 * 3/10 = 2/10)\n",
    "val_text, test_text = train_test_split(val_test_text, random_state=1234, test_size=0.6, stratify=val_test_text['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of the `train_text` data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22225</th>\n",
       "      <td>syrian armi iranian back militia back russian ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29542</th>\n",
       "      <td>initi run megyn kelli sunday newsmagazin show ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15914</th>\n",
       "      <td>21st centuri wire say peopl accept certain amo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21228</th>\n",
       "      <td>wealthi turkish gold trader decis hire former ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12538</th>\n",
       "      <td>dutch businessman convict april sell weapon ex...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10971</th>\n",
       "      <td>kate steinl wrong race kill someon obama advoc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2866</th>\n",
       "      <td>mani articl written georg soro collectivist ac...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40305</th>\n",
       "      <td>china export oil product north korea novemb ch...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33563</th>\n",
       "      <td>donald trump move step closer offici sanction ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3247</th>\n",
       "      <td>britain limit sale sulphur acid outlaw sale co...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31428 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  target\n",
       "22225  syrian armi iranian back militia back russian ...       1\n",
       "29542  initi run megyn kelli sunday newsmagazin show ...       0\n",
       "15914  21st centuri wire say peopl accept certain amo...       0\n",
       "21228  wealthi turkish gold trader decis hire former ...       1\n",
       "12538  dutch businessman convict april sell weapon ex...       1\n",
       "...                                                  ...     ...\n",
       "10971  kate steinl wrong race kill someon obama advoc...       0\n",
       "2866   mani articl written georg soro collectivist ac...       0\n",
       "40305  china export oil product north korea novemb ch...       1\n",
       "33563  donald trump move step closer offici sanction ...       0\n",
       "3247   britain limit sale sulphur acid outlaw sale co...       1\n",
       "\n",
       "[31428 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our baseline model, we will be using the `TF-IDF` Vectorizer to pre-process articles and then apply Logistic Classifier.\n",
    "\n",
    "- **fit_transform()** method learns vocabulary and `IDF` used for both training & test data. Returns document-term matrix with calculated `TF-IDF` values.\n",
    "\n",
    "- **transform()** method uses the vocabulary and document frequencies (df) learned by **fit_transform()**. Returns document-term matrix with calculated `TF-IDF` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note, ngrams = 1, which is the default value if not specified in TfidfVectorizer. \n",
    "text_transformer = TfidfVectorizer(stop_words='english', max_features=1000)\n",
    "\n",
    "# vectorize train and test data. Produce TF-IDF for train data\n",
    "X_train_text = text_transformer.fit_transform(train_text['text'])\n",
    "X_val_text = text_transformer.transform(val_text['text'])\n",
    "X_test_text = text_transformer.transform(test_text['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the example of the stop words used in TfidfVectorizer that will be filtered out from our observations (i.e. articles), both 'training' and 'test':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['000', '10', '100', '11', '12', '13', '14', '15', '16', '17', '18',\n",
       "       '20', '2012', '2013', '2014', '2015', '2016', '2017', '21st',\n",
       "       '21wire', '24', '25', '30', '50', 'abl', 'abort', 'absolut',\n",
       "       'abus', 'accept', 'access', 'accord', 'account', 'accus', 'act',\n",
       "       'action', 'activ', 'activist', 'actual', 'ad', 'addit', 'address',\n",
       "       'administr', 'admit', 'advanc', 'advis', 'affair', 'affect',\n",
       "       'african', 'agenc', 'agenda', 'agent', 'ago', 'agre', 'agreement',\n",
       "       'ahead', 'aid', 'aim', 'air', 'al', 'alleg', 'alli', 'allow',\n",
       "       'alreadi', 'alway', 'ambassador', 'amend', 'america', 'american',\n",
       "       'announc', 'anoth', 'answer', 'anti', 'anyon', 'anyth', 'appar',\n",
       "       'appeal', 'appear', 'appoint', 'approach', 'approv', 'april',\n",
       "       'arab', 'arabia', 'area', 'argu', 'arm', 'armi', 'arrest', 'arriv',\n",
       "       'articl', 'ask', 'assault', 'assist', 'associ', 'attack',\n",
       "       'attempt', 'attend', 'attent', 'attorney', 'august'], dtype=object)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = text_transformer.get_feature_names_out()\n",
    "feature_names[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of observations (articles) in  the train data:  31428\n",
      "The number of features (tokens) in  the train data:  1000\n"
     ]
    }
   ],
   "source": [
    "print('The number of observations (articles) in  the train data: ', X_train_text.shape[0])\n",
    "print('The number of features (tokens) in  the train data: ', X_train_text.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of `TF-IDF` matrix, **val_text**, for the validation dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.02544547, 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.07992456, 0.        , 0.10354717, ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.04468951, 0.05243942, ..., 0.08775583, 0.        ,\n",
       "         0.        ],\n",
       "        ...,\n",
       "        [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ],\n",
       "        [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "         0.        ]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val_text.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the `Logistic Classifier` as our baseline model for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1, multi_class='multinomial', solver='sag')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit = LogisticRegression(penalty = 'l2', C = 1, solver= 'sag', multi_class = 'multinomial')\n",
    "logit.fit(X_train_text, train_text['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have trained our model, we will apply it to predict labels (true/false) for articles in the test data and calculate the accuracy score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'numpy.float64' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/dk/pln7xlc14nzbppm5dwwxnl1h0000gn/T/ipykernel_28450/2668016096.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain_predicted_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_accuracy_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_text\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_predicted_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpredicted_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0maccuracy_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_text\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'numpy.float64' object is not callable"
     ]
    }
   ],
   "source": [
    "train_predicted_label = logit.predict(X_train_text)\n",
    "train_accuracy_score = accuracy_score(train_text['target'], train_predicted_label)\n",
    "\n",
    "predicted_label = logit.predict(X_val_text)\n",
    "accuracy_score = accuracy_score(val_text['target'], predicted_label)\n",
    "\n",
    "print('the accuracy score on the training data is: ', train_accuracy_score)\n",
    "print('the accuracy score on the validation data is: ', accuracy_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Future steps:**\n",
    "\n",
    "- Continue cleaning data with the use of Regex and other packages (digits, punctation, 'Router', '21st century')\n",
    "\n",
    "- Further analysis of data\n",
    "\n",
    "- Re-run model after data is cleaned\n",
    "\n",
    "- Discover options to improve the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes form meeting with Cole:\n",
    "\n",
    "try different models with different # number of features \n",
    "\n",
    "1. spacy for steming/punctation - `done`\n",
    "2. remove article sources - `done`\n",
    "3. try different # of features - `pending`\n",
    "\n",
    "parced list of words\n",
    "\n",
    "analysis: size of trainings \n",
    "distribution of words "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d7660429b82a00d1826214792335f268fe35060b99a4b0365a6603395481d28c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
